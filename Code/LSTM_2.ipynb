{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import random\n",
    "import re\n",
    "import dask.bag as db\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import jieba as jb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(line):\n",
    "    line = str(line)\n",
    "    if line.strip() == '':\n",
    "        return ''\n",
    "    rule = re.compile(\"[^0-9a-zA-Z\\s-]\")\n",
    "    line = rule.sub('', line).strip()\n",
    "    return line\n",
    "\n",
    "# select year for data set\n",
    "def LSTM_2_select_year(year1, year2):\n",
    "\n",
    "    # use dask.bag to load json file\n",
    "    docs = db.read_text('/content/datasets/Cornell-University/arxiv/arxiv-metadata-oai-snapshot.json').map(json.loads)\n",
    "\n",
    "    # Submissions by datetime\n",
    "    get_year = lambda x: x['versions'][-1]['created'].split(' ')[3]\n",
    "\n",
    "    # get only necessary fields\n",
    "    trim = lambda x: {'id': x['id'],\n",
    "                      'title': x['title'],\n",
    "                      'category':x['categories'].split(' '),\n",
    "                      'abstract':x['abstract'],\n",
    "                      'time':x['versions'][-1]['created'].split(' ')[3]}\n",
    "\n",
    "    # filter for papers published on or after ????-01-01\n",
    "    docs_df = (docs.filter(lambda x: int(get_year(x)) > year1)\n",
    "                   .filter(lambda x: int(get_year(x)) < year2)\n",
    "                   .map(trim)\n",
    "                   .compute())\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    docs_df = pd.DataFrame(docs_df)\n",
    "\n",
    "    # add main category\n",
    "    docs_df['main_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x][0].split())\n",
    "    \n",
    "    # add main category 2 \n",
    "    docs_df['main_category2'] = docs_df.main_category.apply(lambda x: x[0])\n",
    "\n",
    "    # discard categories with number of samples < 500\n",
    "    d = {'cat':docs_df['main_category2'].value_counts().index, 'count': docs_df['main_category2'].value_counts()}\n",
    "    df_cat = pd.DataFrame(data=d).reset_index(drop=True)\n",
    "    df_cat.drop(df_cat[df_cat['count']<500].index)\n",
    "    discard_cat = df_cat[df_cat['count']<500].cat.tolist()\n",
    "    docs_df = docs_df.drop(docs_df[(docs_df.main_category2.apply(lambda x: x in discard_cat)==True)].index)\n",
    "\n",
    "    # find all categories\n",
    "    cat_list = docs_df['main_category2'].unique()\n",
    "\n",
    "    return docs_df, cat_list\n",
    "\n",
    "# select categories\n",
    "def LSTM_2_select_categories(docs_df, cat_list, n_topics, num_cal, epochs = 5, batch_size = 64):\n",
    "\n",
    "    # get all the combinations for categories\n",
    "    #list_2 = list(itertools.combinations(cat_list, 2))\n",
    "    list_2 = [('math', 'nlin'), \n",
    "              ('cond-mat', 'hep-ph'),\n",
    "              ('astro-ph', 'math-ph'),\n",
    "              ('stat', 'q-bio'),\n",
    "              ('cs', 'stat'),\n",
    "              ('math', 'math-ph'),\n",
    "              ('physics', 'hep-ex'),\n",
    "              ('nlin', 'q-fin'),\n",
    "              ('hep-ph', 'gr-qc'),\n",
    "              ('nucl-th', 'q-bio')]\n",
    "\n",
    "    m = len(list_2)\n",
    "\n",
    "    # initialize accuracy, var\n",
    "    acc = np.zeros(num_cal)\n",
    "    num = np.zeros(num_cal)\n",
    "    his = []\n",
    "\n",
    "    for i in range(num_cal):\n",
    "        print('pair',i)\n",
    "\n",
    "        # define two categories\n",
    "        cat1 = list_2[i][0]\n",
    "        cat2 = list_2[i][1]\n",
    "\n",
    "        # filter for papers in 'cat1' or 'cat2' area\n",
    "        docs_df2 = docs_df[(docs_df.main_category.apply(lambda x: cat1 in x )==True)|(docs_df.main_category.apply(lambda x: cat2 in x )==True)]\n",
    "\n",
    "        # define stopwords\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "        # remove punctuation\n",
    "        docs_df2['clean_review'] = docs_df2['abstract'].apply(remove_punctuation)\n",
    "\n",
    "        # remove stopwords\n",
    "        docs_df2['cut'] = docs_df2['clean_review'].apply(lambda x: \" \".join([w for w in list(jb.cut(x)) if w not in stopwords]))\n",
    "        docs_df3 = docs_df2[['cut','main_category2']]\n",
    "\n",
    "        # get the number of papers\n",
    "        num[i] = len(docs_df3)\n",
    "\n",
    "        # cat tansfer to id\n",
    "        docs_df3['cat_id'] = docs_df3['main_category2'].factorize()[0]\n",
    "        cat_id_df = docs_df3[['main_category2', 'cat_id']].drop_duplicates().sort_values('cat_id').reset_index(drop=True)\n",
    "        cat_to_id = dict(cat_id_df.values)\n",
    "        id_to_cat = dict(cat_id_df[['cat_id', 'main_category2']].values)\n",
    "\n",
    "        # LSTM\n",
    "        # Set the 50,000 most frequently used words\n",
    "        MAX_NB_WORDS = 50000\n",
    "\n",
    "        # Maximum length of each cut review\n",
    "        MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "        # Set the dimension of the Embeddingceng layer\n",
    "        EMBEDDING_DIM = 100\n",
    "\n",
    "        tokenizer = Tokenizer(num_words = MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "        tokenizer.fit_on_texts(docs_df3['cut'].values)\n",
    "        word_index = tokenizer.word_index\n",
    "        X = tokenizer.texts_to_sequences(docs_df3['cut'].values)\n",
    "\n",
    "        # fill X\n",
    "        X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    " \n",
    "        # get onehot\n",
    "        Y = pd.get_dummies(docs_df3['cat_id']).values\n",
    "\n",
    "        # Split training set and test set\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "\n",
    "        # Define model\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "        model.add(SpatialDropout1D(0.2))\n",
    "        model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "        history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,\n",
    "                            callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "        his.append(history.history)\n",
    "\n",
    "        # Compute test accuracy\n",
    "        acc[i] = min(history.history['val_accuracy'])\n",
    "\n",
    "    return acc, list_2, num, his\n",
    "\n",
    "# 2 topics classification with LSTM\n",
    "def LSTM_2_classification(start, end, n_topics, num_cal, epochs = 5, batch_size = 64):\n",
    "\n",
    "    list_year = np.arange(start, end+1)\n",
    "    n = len(list_year)\n",
    "    names = locals()\n",
    "    his_total = locals()\n",
    "    cat_list = []\n",
    "\n",
    "    for j in range(n):\n",
    "        year1 = list_year[j] - 1\n",
    "        year2 = list_year[j] + 1\n",
    "        docs_df, cat_list_now = LSTM_2_select_year(year1, year2)\n",
    "        names['docs_df'+str(j)] = docs_df\n",
    "    #     if j == 0:\n",
    "    #         cat_list = cat_list_now.tolist()\n",
    "    #     else:\n",
    "    #         cat_list = extra_same_elem(cat_list_now.tolist(), cat_list)\n",
    "    \n",
    "    # cat_list = set(cat_list)\n",
    "    results_acc = np.zeros((n, num_cal))\n",
    "    results_num = np.zeros((n, num_cal))\n",
    "\n",
    "    for k in range(n):\n",
    "        print(k)\n",
    "        docs_df = names['docs_df'+str(k)]\n",
    "        acc, list_2, num, his = LSTM_2_select_categories(docs_df, cat_list, n_topics, num_cal, epochs = 1, batch_size = 64)\n",
    "        print('accuracy', acc)\n",
    "        print('number of papers', num)\n",
    "        results_acc[k,:] = acc\n",
    "        results_num[k,:] = num\n",
    "        his_total['year'+str(k)] = his\n",
    "\n",
    "    return results_acc, list_2, results_num, his_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_acc, list_2, results_num, his_total = LSTM_2_classification(2017, 2021, 2, 10, epochs = 1, batch_size = 64)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8aec77fc3140afcac7269543f2e6cd807e1ee484ebc04069e1d17dc787617ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
