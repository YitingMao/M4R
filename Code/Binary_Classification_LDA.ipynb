{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary_Classification_LDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install 'fsspec>=0.3.3'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En8tjKZ9HmSH",
        "outputId": "63ed9936-c864-4fff-a808-9f1c0e0b71d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fsspec>=0.3.3 in /usr/local/lib/python3.7/dist-packages (2022.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "japG0O1-E60a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import itertools\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.bag as db\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, hamming_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42rncxz0FCfr",
        "outputId": "61c6b31f-45f4-49ec-cb77-f090f87c30f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "token = {\"username\":\"karinmao\",\"key\":\"0145306b9df944a40f90e51f15167f9c\"}\n",
        "with open('/content/kaggle.json','w') as file:\n",
        "  json.dump(token,file)"
      ],
      "metadata": {
        "id": "oS4Zg5ZjGo5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v /content\n",
        "!kaggle datasets download -d Cornell-University/arxiv\n",
        "!unzip -uq \"/content/datasets/Cornell-University/arxiv/arxiv.zip\" -d \"/content/datasets/Cornell-University/arxiv\""
      ],
      "metadata": {
        "id": "rOckfcwvGrYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f89466f-d85c-42d9-a4a6-105501805689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- path is now set to: /content\n",
            "arxiv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "docs = db.read_text('/content/datasets/Cornell-University/arxiv/arxiv-metadata-oai-snapshot.json').map(json.loads)"
      ],
      "metadata": {
        "id": "gRx9iY5tG58m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''variables: year, cat1, cat2'''\n",
        "'''structure: Data preprocessing + Topic extraction with LatentDirichletAllocation'''\n",
        "\n",
        "# Extract common elements from multiple lists\n",
        "def extra_same_elem(lst, lst2):\n",
        "\n",
        "    iset = set(lst)\n",
        "    s = set(lst2)\n",
        "    iset = iset.intersection(s)\n",
        "\n",
        "    return list(iset)\n",
        "\n",
        "# get top words\n",
        "def top_words_data_frame(model: LatentDirichletAllocation, tf_idf_vectorizer: TfidfVectorizer, n_top_words: int) -> pd.DataFrame:\n",
        "\n",
        "    rows = []\n",
        "    feature_names = tf_idf_vectorizer.get_feature_names()\n",
        "\n",
        "    for topic in model.components_:\n",
        "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "        rows.append(top_words)\n",
        "\n",
        "    columns = [f'topic {i+1}' for i in range(n_top_words)]\n",
        "    df = pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to plot the most relevant words\n",
        "def plot_top_words(model, feature_names, n_top_words, title):\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(30, 15), sharex=True)\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
        "        top_features = [feature_names[i] for i in top_features_ind]\n",
        "        weights = topic[top_features_ind]\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_features, weights, height=0.7)\n",
        "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
        "        ax.invert_yaxis()\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
        "        \n",
        "        for i in \"top right left\".split():\n",
        "            ax.spines[i].set_visible(False)\n",
        "            \n",
        "        fig.suptitle(title, fontsize=40)\n",
        "\n",
        "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# select year for data set\n",
        "def LDA_2_select_year(year1, year2):\n",
        "\n",
        "    # use dask.bag to load json file\n",
        "    docs = db.read_text('/content/datasets/Cornell-University/arxiv/arxiv-metadata-oai-snapshot.json').map(json.loads)\n",
        "\n",
        "    # Submissions by datetime\n",
        "    get_year = lambda x: x['versions'][-1]['created'].split(' ')[3]\n",
        "\n",
        "    # get only necessary fields\n",
        "    trim = lambda x: {'id': x['id'],\n",
        "                      'title': x['title'],\n",
        "                      'category':x['categories'].split(' '),\n",
        "                      'abstract':x['abstract'],\n",
        "                      'time':x['versions'][-1]['created'].split(' ')[3]}\n",
        "\n",
        "    # filter for papers published on or after ????-01-01\n",
        "    docs_df = (docs.filter(lambda x: int(get_year(x)) > year1)\n",
        "                   .filter(lambda x: int(get_year(x)) < year2)\n",
        "                   .map(trim)\n",
        "                   .compute())\n",
        "\n",
        "    # convert to pandas dataframe\n",
        "    docs_df = pd.DataFrame(docs_df)\n",
        "\n",
        "    # add main category\n",
        "    docs_df['main_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x][0].split())\n",
        "    \n",
        "    # add main category 2 \n",
        "    docs_df['main_category2'] = docs_df.main_category.apply(lambda x: x[0])\n",
        "\n",
        "    # discard categories with number of samples < 500\n",
        "    d = {'cat':docs_df['main_category2'].value_counts().index, 'count': docs_df['main_category2'].value_counts()}\n",
        "    df_cat = pd.DataFrame(data=d).reset_index(drop=True)\n",
        "    df_cat.drop(df_cat[df_cat['count']<500].index)\n",
        "    discard_cat = df_cat[df_cat['count']<500].cat.tolist()\n",
        "    docs_df = docs_df.drop(docs_df[(docs_df.main_category2.apply(lambda x: x in discard_cat)==True)].index)\n",
        "\n",
        "    # find all categories\n",
        "    cat_list = docs_df['main_category2'].unique()\n",
        "\n",
        "    return docs_df, cat_list\n",
        "\n",
        "# select categories\n",
        "def LDA_2_select_categories(docs_df, cat_list, n_topics, num_cal):\n",
        "\n",
        "    #num = 19\n",
        "    # get all the combinations for categories\n",
        "    #list_2 = list(itertools.combinations(cat_list, 2))\n",
        "    list_2 = [('cs', 'math'),\n",
        "              ('cs', 'cond-mat'),\n",
        "              ('cs', 'astro-ph'),\n",
        "              ('cs', 'physics'),\n",
        "              ('math', 'cond-mat'),\n",
        "              ('math', 'astro-ph'),\n",
        "              ('math', 'physics'),\n",
        "              ('cond-mat', 'astro-ph'),\n",
        "              ('cond-mat', 'physics'),\n",
        "              ('astro-ph', 'physics')]\n",
        "\n",
        "    m = len(list_2)\n",
        "\n",
        "    # initialize accuracy, var\n",
        "    acc = np.zeros(num_cal)\n",
        "    num = np.zeros(num_cal)\n",
        "\n",
        "    for i in range(num_cal):\n",
        "        print(list_2[i])\n",
        "\n",
        "        # define two categories\n",
        "        cat1 = list_2[i][0]\n",
        "        cat2 = list_2[i][1]\n",
        "\n",
        "        # filter for papers in 'cat1' or 'cat2' area\n",
        "        docs_df2 = docs_df[(docs_df.main_category.apply(lambda x: cat1 in x )==True)|(docs_df.main_category.apply(lambda x: cat2 in x )==True)]\n",
        "\n",
        "        # convert general category into label columns\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        labels = mlb.fit_transform(docs_df2.main_category)\n",
        "\n",
        "        # concatenate with the abstracts\n",
        "        df = pd.concat([docs_df2[['abstract','title']].reset_index(drop=True), pd.DataFrame(labels)], axis=1)\n",
        "        df.columns = ['abstract','title'] + list(mlb.classes_)\n",
        "\n",
        "        # get the minimum length of two categories\n",
        "        sample_num = min(len(df.loc[df[cat1] == 1]), len(df.loc[df[cat2] == 1]))\n",
        "        num[i] = sample_num\n",
        "\n",
        "        # sampling\n",
        "        df_cat1 = df.loc[df[cat1] == 1].sample(n=sample_num)\n",
        "        df_cat2 = df.loc[df[cat2] == 1].sample(n=sample_num)\n",
        "        sample_df = shuffle(pd.concat([df_cat1,df_cat2],axis=0))\n",
        "\n",
        "        # remove patterns\n",
        "        pattern = u'[\\\\s\\\\d,.<>/?:;\\'\\\"[\\\\]{}()\\\\|~!\\t\"@#$%^&*\\\\-_=+\\n《》、？：；“”‘’｛}（）…￥！—┄－]+'\n",
        "        sample_df['cut'] = (sample_df['abstract']\n",
        "                            .apply(lambda x: str(x))\n",
        "                            .apply(lambda x: re.sub(pattern, ' ', x)))\n",
        "\n",
        "        # Use tf features to vectorize the abstacrts for LDA\n",
        "        tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1, stop_words=\"english\")\n",
        "        tf = tf_vectorizer.fit_transform(sample_df['cut'])\n",
        "\n",
        "        # list of feature names\n",
        "        feature_names = tf_vectorizer.get_feature_names()\n",
        "\n",
        "        # feature names TF matrix\n",
        "        matrix = tf.toarray()\n",
        "        feature_names_df = pd.DataFrame(matrix,columns=feature_names)\n",
        "\n",
        "        # Fit the LDA model\n",
        "        lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50, learning_method='online', learning_offset=50., random_state=0)\n",
        "        lda.fit(tf)\n",
        "\n",
        "        # Compute test accuracy\n",
        "        y = sample_df[cat1]\n",
        "        pred = lda.transform(tf).argmax(axis=1)\n",
        "        prob = lda.transform(tf)\n",
        "        acc[i] = max(accuracy_score(y,pred),1-accuracy_score(y,pred))\n",
        "\n",
        "    return acc, list_2, num\n",
        "\n",
        "# 2 topics classification with LatentDirichletAllocation\n",
        "def LDA_2_classification(start, end, n_topics, num_cal):\n",
        "\n",
        "    list_year = np.arange(start, end+1)\n",
        "    n = len(list_year)\n",
        "    names = locals()\n",
        "    cat_list = []\n",
        "\n",
        "    for j in range(n):\n",
        "        year1 = list_year[j] - 1\n",
        "        year2 = list_year[j] + 1\n",
        "        docs_df, cat_list_now = LDA_2_select_year(year1, year2)\n",
        "        names['docs_df'+str(j)] = docs_df\n",
        "    \n",
        "    results_acc = np.zeros((n, num_cal))\n",
        "    results_num = np.zeros((n, num_cal))\n",
        "    # print('total length', len(cat_list))\n",
        "    # print(cat_list)\n",
        "\n",
        "    for k in range(n):\n",
        "        print(k)\n",
        "        docs_df = names['docs_df'+str(k)]\n",
        "        acc, list_2, num = LDA_2_select_categories(docs_df, cat_list, n_topics, num_cal)\n",
        "        print('accuracy', acc)\n",
        "        print('number of papers', num)\n",
        "        results_acc[k,:] = acc\n",
        "        results_num[k,:] = num\n",
        "\n",
        "    return results_acc, list_2, results_num"
      ],
      "metadata": {
        "id": "u7ZQbXIcH60X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LDA_2_classification(year, n_topics):\n",
        "\n",
        "    list_2 = [('cs', 'math'),\n",
        "              ('cs', 'cond-mat'),\n",
        "              ('cs', 'astro-ph'),\n",
        "              ('cs', 'physics'),\n",
        "              ('math', 'cond-mat'),\n",
        "              ('math', 'astro-ph'),\n",
        "              ('math', 'physics'),\n",
        "              ('cond-mat', 'astro-ph'),\n",
        "              ('cond-mat', 'physics'),\n",
        "              ('astro-ph', 'physics')]\n",
        "\n",
        "    # use dask.bag to load json file\n",
        "    docs = db.read_text('/content/datasets/Cornell-University/arxiv/arxiv-metadata-oai-snapshot.json').map(json.loads)\n",
        "\n",
        "    # Submissions by datetime\n",
        "    get_year = lambda x: x['versions'][-1]['created'].split(' ')[3]\n",
        "\n",
        "    # get only necessary fields\n",
        "    trim = lambda x: {'id': x['id'],\n",
        "                      'title': x['title'],\n",
        "                      'category':x['categories'].split(' '),\n",
        "                      'abstract':x['abstract'],\n",
        "                      'time':x['versions'][-1]['created'].split(' ')[3]}\n",
        "\n",
        "    # filter for papers published on or after ????-01-01\n",
        "    docs_df = (docs.filter(lambda x: int(get_year(x)) > (year-1))\n",
        "                   .filter(lambda x: int(get_year(x)) < (year+1))\n",
        "                   .map(trim)\n",
        "                   .compute())\n",
        "\n",
        "    # convert to pandas dataframe\n",
        "    docs_df = pd.DataFrame(docs_df)\n",
        "\n",
        "    # add main category\n",
        "    docs_df['main_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x][0].split())\n",
        "    \n",
        "    # add main category 2 \n",
        "    docs_df['main_category2'] = docs_df.main_category.apply(lambda x: x[0])\n",
        "\n",
        "    # discard categories with number of samples < 500\n",
        "    d = {'cat':docs_df['main_category2'].value_counts().index, 'count': docs_df['main_category2'].value_counts()}\n",
        "    df_cat = pd.DataFrame(data=d).reset_index(drop=True)\n",
        "    df_cat.drop(df_cat[df_cat['count']<500].index)\n",
        "    discard_cat = df_cat[df_cat['count']<500].cat.tolist()\n",
        "    docs_df = docs_df.drop(docs_df[(docs_df.main_category2.apply(lambda x: x in discard_cat)==True)].index)\n",
        "\n",
        "    # find all categories\n",
        "    cat_list = docs_df['main_category2'].unique()\n",
        "\n",
        "\n",
        "    # initialize accuracy, var\n",
        "    acc = np.zeros(len(list_2))\n",
        "    num = np.zeros(len(list_2))\n",
        "\n",
        "    for i in range(len(list_2)):\n",
        "        \n",
        "        print(list_2[i])\n",
        "\n",
        "        # define two categories\n",
        "        cat1 = list_2[i][0]\n",
        "        cat2 = list_2[i][1]\n",
        "\n",
        "        # filter for papers in 'cat1' or 'cat2' area\n",
        "        docs_df2 = docs_df[(docs_df.main_category.apply(lambda x: cat1 in x )==True)|(docs_df.main_category.apply(lambda x: cat2 in x )==True)]\n",
        "\n",
        "        # convert general category into label columns\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        labels = mlb.fit_transform(docs_df2.main_category)\n",
        "\n",
        "        # concatenate with the abstracts\n",
        "        df = pd.concat([docs_df2[['abstract','title']].reset_index(drop=True), pd.DataFrame(labels)], axis=1)\n",
        "        df.columns = ['abstract','title'] + list(mlb.classes_)\n",
        "\n",
        "        # get the minimum length of two categories\n",
        "        sample_num = min(len(df.loc[df[cat1] == 1]), len(df.loc[df[cat2] == 1]))\n",
        "        num[i] = sample_num\n",
        "\n",
        "        # sampling\n",
        "        df_cat1 = df.loc[df[cat1] == 1].sample(n=sample_num)\n",
        "        df_cat2 = df.loc[df[cat2] == 1].sample(n=sample_num)\n",
        "        sample_df = shuffle(pd.concat([df_cat1,df_cat2],axis=0))\n",
        "\n",
        "        # remove patterns\n",
        "        pattern = u'[\\\\s\\\\d,.<>/?:;\\'\\\"[\\\\]{}()\\\\|~!\\t\"@#$%^&*\\\\-_=+\\n《》、？：；“”‘’｛}（）…￥！—┄－]+'\n",
        "        sample_df['cut'] = (sample_df['abstract']\n",
        "                            .apply(lambda x: str(x))\n",
        "                            .apply(lambda x: re.sub(pattern, ' ', x)))\n",
        "\n",
        "        # Use tf features to vectorize the abstacrts for LDA\n",
        "        # tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1, stop_words=\"english\")\n",
        "        # tf = tf_vectorizer.fit_transform(sample_df['cut'])\n",
        "        tr_ = TfidfVectorizer(max_features = 100, stop_words=\"english\")\n",
        "        tfidf = tr_.fit_transform(sample_df['cut'])\n",
        "\n",
        "        # list of feature names\n",
        "        #feature_names = tf_vectorizer.get_feature_names()\n",
        "\n",
        "        # feature names TF matrix\n",
        "        matrix = tfidf.toarray()\n",
        "        #feature_names_df = pd.DataFrame(matrix,columns=feature_names)\n",
        "\n",
        "        # Fit the LDA model\n",
        "        lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50, learning_method='online', learning_offset=50., random_state=0)\n",
        "        lda.fit(tfidf)\n",
        "\n",
        "        # Compute test accuracy\n",
        "        y = sample_df[cat1]\n",
        "        pred = lda.transform(tfidf).argmax(axis=1)\n",
        "        prob = lda.transform(tfidf)\n",
        "        acc[i] = max(accuracy_score(y,pred),1-accuracy_score(y,pred))\n",
        "\n",
        "    return acc, num\n"
      ],
      "metadata": {
        "id": "dG7X908nk9Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc, num = LDA_2_classification(2021,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nomq2wcDItx8",
        "outputId": "adf30bfe-d39c-459d-beb0-5d6c4c58ce5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('cs', 'math')\n",
            "('cs', 'cond-mat')\n",
            "('cs', 'astro-ph')\n",
            "('cs', 'physics')\n",
            "('math', 'cond-mat')\n",
            "('math', 'astro-ph')\n",
            "('math', 'physics')\n",
            "('cond-mat', 'astro-ph')\n",
            "('cond-mat', 'physics')\n",
            "('astro-ph', 'physics')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(acc)\n",
        "print(num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLY4wIo_SHBI",
        "outputId": "757a2ca4-70b2-4fb5-ff0c-c70d160a5b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.86843324 0.93544858 0.9287536  0.84159461 0.91506392 0.92191996\n",
            " 0.85824198 0.89504316 0.66422687 0.81197658]\n",
            "[43172. 17366. 15292. 14687. 17366. 15292. 14687. 15292. 14687. 14687.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num = 10\n",
        "fig = plt.figure(figsize=(16,6))\n",
        "x = np.arange(num)\n",
        "for i in range(5):\n",
        "    plt.plot(x,results_acc[i],'o',linestyle='dashed')\n",
        "plt.xlabel(list_2[:num])\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(np.arange(2017,2022))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GXiDNmazsBIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1 = results_num.mean(axis=0)\n",
        "y2 = results_acc.std(axis=0)\n",
        "y3 = results_acc.var(axis=0)\n",
        "y4 = results_acc.mean(axis=0)"
      ],
      "metadata": {
        "id": "tiUkvnkGumHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfy1 = pd.DataFrame(y1)\n",
        "dfy2 = pd.DataFrame(y2)\n",
        "dfy3 = pd.DataFrame(y3)\n",
        "dfy4 = pd.DataFrame(y4)\n",
        "y1y2 = pd.concat([dfy1, dfy2, dfy3, dfy4], axis=1)\n",
        "y1y2.columns = ['num', 'std', 'var', 'mean']\n",
        "y1y2.sort_values('num', inplace=True)\n",
        "y1y2"
      ],
      "metadata": {
        "id": "3yaRzA4pxeg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1y2.plot(x = 'num', y = 'var', kind = 'line')\n",
        "plt.xlabel('number of samples')\n",
        "plt.ylabel('variance')"
      ],
      "metadata": {
        "id": "nkb3VcQuwZ0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1y2.plot(x = 'num', y = 'std', kind = 'line')\n",
        "plt.xlabel('number of samples')\n",
        "plt.ylabel('standard deviation')"
      ],
      "metadata": {
        "id": "Apu2BWYOzq5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}