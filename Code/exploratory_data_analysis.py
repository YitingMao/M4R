# -*- coding: utf-8 -*-
"""Exploratory Data Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cv9mu4ioim1WWuvp8NYKtbIhNt6K6YAT

# Exploratory Data Analysis
By Yiting Mao

Mathematics Department, Imperial College London

In this jupyter notebook, I will do the **Exploratory Data Analysis** to the Dataset used in this M4R project ([the arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv)).
"""

!pip install kaggle
!pip install 'fsspec>=0.3.3'

import tensorflow as tf
import torch
import json
import dask.bag as db
import pandas as pd
import numpy as np
import altair as alt
from sklearn.preprocessing import MultiLabelBinarizer
import plotly.express as px
import matplotlib.pyplot as plt
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
import jieba as jb
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA

# Get the GPU device name.
device_name = tf.test.gpu_device_name()

# The device name should look like the following:
if device_name == '/device:GPU:0':
    print('Found GPU at: {}'.format(device_name))
else:
    raise SystemError('GPU device not found')

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

"""## 1.Load arXiv Dataset"""

token = {"username":"karinmao","key":"0145306b9df944a40f90e51f15167f9c"}
with open('/content/kaggle.json','w') as file:
  json.dump(token,file)

!mkdir -p ~/.kaggle
!cp /content/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle config set -n path -v /content
!kaggle datasets download -d Cornell-University/arxiv
!unzip -uq "/content/datasets/Cornell-University/arxiv/arxiv.zip" -d "/content/datasets/Cornell-University/arxiv"

# load data
docs = db.read_text('/content/datasets/Cornell-University/arxiv/arxiv-metadata-oai-snapshot.json').map(json.loads)

"""## 2. Exploratory Data Analysis"""

# number of samples in the dataset :  2075742 
docs.count().compute()

# Example document
docs.take(1)

# Top Submitters
docs.map(lambda x: x['submitter']).frequencies(sort = True).topk(10, key=1).compute()

# Top Authors
parse_authors = trim = lambda x: [' '.join(a).strip() for a in x['authors_parsed']]
docs.map(parse_authors).flatten().frequencies(sort = True).topk(20, key=1).compute()

# Submissions by datetime
get_latest_version = lambda x: x['versions'][-1]['created']

dates = (docs
         .map(get_latest_version)
         .frequencies(sort = True))

# show top submissions datetime
(dates.topk(10, key=1)
      .compute())

# convert to dataframe 
dates = dates.to_dataframe(columns = ['submission_datetime','submissions']).compute()
dates.head(4)

extract_latest_version = lambda x:x['versions'][-1]["created"] ## Here -1 indicates the last element in the versions. 
extract_latest_version_year = lambda x:x['versions'][-1]["created"].split(" ")[3]

pub_by_year = docs.map(extract_latest_version_year).frequencies().to_dataframe(columns=['submission_year','num_submissions']).compute()

pub_by_year=pub_by_year.sort_values(by="submission_year")
pub_by_year.head()

px.line(x='submission_year',y='num_submissions',data_frame=pub_by_year,title="Distribution of Paper Published on Arxiv By Year")

"""Take the samples from 2021."""

# define target year
year = 2021

# Submissions by datetime
get_year = lambda x: x['versions'][-1]['created'].split(' ')[3]

# get only necessary fields
trim = lambda x: {'id': x['id'],
                  'title': x['title'],
                  'category':x['categories'].split(' '),
                  'abstract':x['abstract']}

# filter for papers published on or after ????-01-01
docs_df = (docs.filter(lambda x: int(get_year(x)) > (year-1))
               .filter(lambda x: int(get_year(x)) < (year+1))
               .map(trim)
               .compute())

# convert to pandas
docs_df = pd.DataFrame(docs_df)

# add general category. we are going to use as our target variable
docs_df['general_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x])

docs_df.info()

# add main category
docs_df['main_category'] = docs_df.category.apply(lambda x:[a.split('.')[0] for a in x][0].split())

# add main category 2 
docs_df['main_category2'] = docs_df.main_category.apply(lambda x: x[0])

d = {'cat':docs_df['main_category2'].value_counts().index, 'count': docs_df['main_category2'].value_counts()}
df_cat = pd.DataFrame(data=d).reset_index(drop=True)

# plot samples - categories
fig = plt.figure(figsize=(60,10))
df_cat.plot('cat','count',kind='bar')
plt.xlabel('Categories')
plt.ylabel('Number of samples')
plt.title('year = %s'%(year))

# remove patterns
pattern = u'[\\s\\d,.<>/?:;\'\"[\\]{}()\\|~!\t"@#$%^&*\\-_=+\n《》、？：；“”‘’｛}（）…￥！—┄－]+'
docs_df['cut'] = (docs_df['abstract']
                    .apply(lambda x: str(x))
                    .apply(lambda x: re.sub(pattern, ' ', x)))
# define stopwords
stopwords = nltk.corpus.stopwords.words("english")

# remove stopwords
docs_df['cut2'] = docs_df['cut'].apply(lambda x: " ".join([w for w in list(jb.cut(x)) if w not in stopwords]))

docs_df.head(2)

docs_df.iloc[3].abstract

docs_df.iloc[3].cut2

mlb = MultiLabelBinarizer()
labels = mlb.fit_transform(docs_df.general_category)

# concatenate with the abstracts
df = pd.concat([docs_df[['cut2','title']], pd.DataFrame(labels)], axis=1)
df.columns = ['abstract','title'] + list(mlb.classes_)

# sample and keep columns that have at least 1 positive example
sample_df = df.sample(frac = 1, random_state = 4)
keep = sample_df.iloc[:,2:].apply(sum) > 1
sample_df = pd.concat([sample_df.iloc[:,:2],sample_df.iloc[:,2:].iloc[:,keep.values]], axis = 1)

categories = sample_df.columns[2:]

# removed categories
print('Removed following categories from training : {}'.format(str(keep[~keep].index.to_list())))

sample_pca = sample_df.loc[sample_df.iloc[:,2:].apply(sum, axis=1) == 1,:].sample(n = 15_000, random_state = 4)
tr = TfidfVectorizer(max_features = 1000, stop_words=stop_words).fit_transform(sample_pca.abstract).todense()
# tfidf = tr.fit_transform(docs_df['cut2']).toarray()
pca = PCA(n_components=2).fit(tr)
data2D = pd.DataFrame(pca.transform(tr), columns = ['PC1','PC2'])
# get categories for abstracts
color = sample_pca.iloc[:,2:].apply(lambda x: x.index[x>0][0], axis = 1)
data2D['category'] = color.to_list()
alt.data_transformers.disable_max_rows()
# plot scatter plot 
alt.Chart(data2D).mark_circle(size=15).encode(
    x=alt.X('PC1', axis = None),
    y=alt.Y('PC2', axis = None),
    color = 'category'
).properties(
    title='First 2 Principal Components of the submissions in year 2021',
    width = 600,
    height = 600
).configure_axis(
    grid=False
).configure_view(strokeOpacity=0)

"""## References:
1. [Kaggle - EDA and Multi Label Classification for arXiv](https://www.kaggle.com/code/kobakhit/eda-and-multi-label-classification-for-arxiv)
2. [Exploring ArXiv Using Dask](https://www.kaggle.com/code/aiswaryaramachandran/exploring-arxiv-using-dask/notebook)
"""